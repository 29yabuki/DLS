{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "215954a2-74e4-4d78-a036-3bd3fd819257",
   "metadata": {},
   "source": [
    "# TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78f9559e-5fb8-43cc-bfa9-b726402880a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "044b11a2-7bae-426b-961c-a04c4d3ab6cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.20.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__ # check tf ver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54792664-8345-4fe3-9c5c-1c7b07133cb3",
   "metadata": {},
   "source": [
    "## Simple cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b8dcb55-ae0d-4883-a557-6533911ad592",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = tf.Variable(0, dtype=tf.float32)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a9901b-42c5-4e75-bacf-b177ca97f1f4",
   "metadata": {},
   "source": [
    "In TensorFlow, only the forward propagation is coded. TF automatically computes back prop. One way to do this is with `tf.GradientTape`.\n",
    "\n",
    "`tf.GradientTape`:\n",
    "- records operations during forward prop.\n",
    "- when played back in reverse, it automatically computes back prop and grads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10a3d253-c505-46c8-80b0-38a31628b9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step():\n",
    "    with tf.GradientTape() as tape:\n",
    "        cost = w ** 2 - 10 * w + 25\n",
    "    trainable_variables = [w]\n",
    "    grads = tape.gradient(cost, trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3be339-56a6-4139-8840-6d22ab1c0639",
   "metadata": {},
   "source": [
    "Given a cost function $J = w^2 - 10w + 25$, the value that minimizes $J$ is $w = 5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da23e5b9-0c04-4d57-a9b6-8415c13a5999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w # initial value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a7dba8e-02ab-47f2-95a9-b61547243b07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.0999993085861206>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_step() # run one step\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f480c106-df1c-4262-85b7-e58c78cd1c85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=5.000000953674316>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(1000): # run 1000 steps\n",
    "    train_step()\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34abe600-20ce-4144-8f5c-c3ffc334241b",
   "metadata": {},
   "source": [
    "## Extending to train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c3e7c21-98eb-4242-9778-ca7ff9861047",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = tf.Variable(0, dtype=tf.float32)\n",
    "x = np.array([1.0, -10.0, 25.0], dtype=np.float32)\n",
    "optimizer = tf.keras.optimizers.Adam(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b065afef-7986-4b81-a0ff-a023da0e8cc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cost_fn(x):\n",
    "    return (x[0] * w ** 2) + (x[1] * w) + x[2]\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c389e889-22eb-46e2-9ca1-05b66632d168",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(x, w, optimizer):\n",
    "    with tf.GradientTape() as tape:\n",
    "        cost = cost_fn(x)\n",
    "    trainable_variables = [w]\n",
    "    grads = tape.gradient(cost, trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4a50ced-6768-4262-9822-eea83c97a1ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.0999993085861206>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_step(x, w, optimizer)\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5014d9b5-3608-4c27-bc87-dd7e873ba14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(x, w, optimizer):\n",
    "    for i in range(1000):\n",
    "        train_step(x, w, optimizer)\n",
    "    return w\n",
    "w = training(x, w, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc70403e-07cc-4bc6-a290-a59f426b022b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=5.000000953674316>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fd2173-ea63-4809-96b5-6e4874a5a27f",
   "metadata": {},
   "source": [
    "## Linear function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11861e46-f7a7-4baa-b273-65d706674716",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_function():\n",
    "    X = tf.constant(np.random.randn(3, 1), name='X')\n",
    "    W = tf.Variable(np.random.randn(4, 3), name='W')\n",
    "    b = tf.Variable(np.random.randn(4, 1), name='b')\n",
    "    Y = tf.add(tf.matmul(W, X), b)\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a212d7-8a08-4ee8-ae1e-1f2ee6ef3033",
   "metadata": {},
   "source": [
    "## Sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "411c29eb-9ad5-4112-94ac-be3da862937d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: sigmoid\n",
    "\n",
    "def sigmoid(z):\n",
    "    z = tf.cast(z, tf.float32)\n",
    "    a = tf.keras.activations.sigmoid(z)\n",
    "    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bcb08b-74c3-40a7-a9c8-ce94fc968e05",
   "metadata": {},
   "source": [
    "## One hot encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b632ff0-9e77-4411-a122-69114d55b083",
   "metadata": {},
   "source": [
    "Often times, you will have a  $Y$ vector with numbers ranging from $(0, C-1)$.\n",
    "\n",
    "For example, if $C = 4$:\n",
    "$$\n",
    "y = [1 \\; 2 \\; 3 \\; 0 \\; 2 \\; 1] \\ \\text{is often converted to}\n",
    "\\begin{bmatrix}\n",
    "0 & 0 & 0 & 1 & 0 & 1 \\\\\n",
    "1 & 0 & 0 & 0 & 0 & 1 \\\\\n",
    "0 & 1 & 0 & 0 & 1 & 0 \\\\\n",
    "0 & 0 & 1 & 0 & 0 & 0 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Rows 1 through 4 correspond to classes 0, 1, 2, and 3, respectively.\n",
    "\n",
    "This is _one hot_ encoding. In the converted representation, exactly one element of each column is _hot_â€”set to 1.\n",
    "\n",
    "In tensorflow, use the ff. line of code to implement one hot encoding: `tf.one_hot(labels, depth, axis=0)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c35206c-360b-410f-a60c-71848485f3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_matrix(label, C=6):\n",
    "    one_hot = tf.reshape(tf.one_hot(label, C, axis=0), shape=[C, ])\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ca155f-363a-4a4a-b15f-83984a4fe186",
   "metadata": {},
   "source": [
    "## Parameter initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60dc8333-7a45-4c25-b7e4-83f299af1912",
   "metadata": {},
   "source": [
    "Initializing parameters using the Glorot (Xavier) initializer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "34721bed-295b-44e1-9129-a96b0283b84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters():\n",
    "    W1 = tf.Variable(initializer((25, 12288)))\n",
    "    b1 = tf.Variable(initializer((25, 1)))\n",
    "    W2 = tf.Variable(initializer((12, 25)))\n",
    "    b2 = tf.Variable(initializer((12, 1)))\n",
    "    W3 = tf.Variable(initializer((6, 12)))\n",
    "    b3 = tf.Variable(initializer((6, 1)))\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2,\n",
    "                  \"W3\": W3,\n",
    "                  \"b3\": b3}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7625b85d-895f-4ed4-a909-f0bb0517a246",
   "metadata": {},
   "source": [
    "## Forward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274f3669-554f-43f6-8f5d-ac905e1023fc",
   "metadata": {},
   "source": [
    "For TensorFlow, you only need to implement the forward prop function. \n",
    "\n",
    "It will keep track of the operations you did to calculate the back prop automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c61147e-2052-4f95-9a77-0138d624738f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "    \n",
    "    Z1 = tf.math.add(tf.linalg.matmul(W1, X), b1)\n",
    "    A1 = tf.keras.activations.relu(Z1)\n",
    "    Z2 = tf.math.add(tf.linalg.matmul(W2, A1), b2)\n",
    "    A2 = tf.keras.activations.relu(Z2)\n",
    "    Z3 = tf.math.add(tf.linalg.matmul(W3, A2), b3)\n",
    "    \n",
    "    return Z3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4997e5d2-7022-425b-a895-a697714c3bc9",
   "metadata": {},
   "source": [
    "## Total loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4678cab-0e57-4cae-8b74-a857e1ce6c70",
   "metadata": {},
   "source": [
    "When the classification has multiple labels, categorical cross-entropy should be the loss function.\n",
    "\n",
    "Cost is normally computed as the sum of losses over all samples, divided by the total number of samples.\n",
    "\n",
    "Using the total loss (and not the mean loss) per mini-batch ensures consistency in the final cost value.\n",
    "\n",
    "Additional remarks:\n",
    "- `y_pred`\" and \"`y_true`\" inputs of [tf.keras.losses.categorical_crossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/categorical_crossentropy) are expected to be of shape (number of examples, num_classes).\n",
    "- `tf.reduce_sum` does the summation over the examples.\n",
    "- Softmax will be taken care by `tf.keras.losses.categorical_crossentropy` by setting its parameter `from_logits=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d244ec69-1100-49bd-b536-a4332579cedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compute_total_loss \n",
    "\n",
    "def compute_total_loss(logits, labels):\n",
    "    loss = tf.keras.losses.categorical_crossentropy(tf.transpose(labels), \n",
    "                                                    tf.transpose(logits), \n",
    "                                                    from_logits=True)\n",
    "    total_loss = tf.reduce_sum(loss)\n",
    "    \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc12e5c-2d1f-464b-8bfc-e7ac30b2ad5c",
   "metadata": {},
   "source": [
    "## Model example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa49a6b-e269-47bf-8f52-408eaee96749",
   "metadata": {},
   "source": [
    "Specify an optimizer in a single line (e.g., `tf.keras.optimizers.Adam`). Then, call it within the training loop.\n",
    "\n",
    "`tape.gradient` function retrives the gradients of recorded operations with automatic differentiation inside a `GradientTape` block.\n",
    "\n",
    "`optimizer.apply_gradients` applies the optimizer's update rules to each trainable parameter.\n",
    "\n",
    "Use `dataset = dataset.prefetch(8)` to avoid memory bottlenecks when reading data from disk.\n",
    "\n",
    "`prefetch()` prepares a portion of data in advance for the next step. Because, the iteration is streamingâ€”the data is does not need to fit into the memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7eeada3f-0984-46ff-981b-69f87a2f47d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.0001,\n",
    "          num_epochs = 1500, minibatch_size = 32, print_cost = True):\n",
    "    \"\"\"\n",
    "    Implements a three-layer tensorflow neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SOFTMAX.\n",
    "    \"\"\"\n",
    "    \n",
    "    costs = [] # keep track of the cost\n",
    "    train_acc = []\n",
    "    test_acc = []\n",
    "    \n",
    "    # initialize params\n",
    "    parameters = initialize_parameters()\n",
    "\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "    \n",
    "    # CategoricalAccuracy will track the accuracy for this multiclass problem\n",
    "    test_accuracy = tf.keras.metrics.CategoricalAccuracy()\n",
    "    train_accuracy = tf.keras.metrics.CategoricalAccuracy()\n",
    "    \n",
    "    dataset = tf.data.Dataset.zip((X_train, Y_train))\n",
    "    test_dataset = tf.data.Dataset.zip((X_test, Y_test))\n",
    "    \n",
    "    # get the num of elements of a dataset using the cardinality method\n",
    "    m = dataset.cardinality().numpy()\n",
    "    \n",
    "    minibatches = dataset.batch(minibatch_size).prefetch(8)\n",
    "    test_minibatches = test_dataset.batch(minibatch_size).prefetch(8)\n",
    "    X_train = X_train.batch(minibatch_size, drop_remainder=True).prefetch(8) # extra step    \n",
    "    Y_train = Y_train.batch(minibatch_size, drop_remainder=True).prefetch(8) # loads memory faster \n",
    "\n",
    "    # train loop\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        epoch_total_loss = 0.\n",
    "        \n",
    "        # reset object to start measuring (from 0) the accuracy each epoch\n",
    "        train_accuracy.reset_states()\n",
    "        \n",
    "        for (minibatch_X, minibatch_Y) in minibatches:\n",
    "            \n",
    "            with tf.GradientTape() as tape:\n",
    "                Z3 = forward_propagation(tf.transpose(minibatch_X), parameters) # predict\n",
    "                minibatch_total_loss = compute_total_loss(Z3, tf.transpose(minibatch_Y)) # loss\n",
    "\n",
    "            # accumulate the accuracy of all the batches\n",
    "            train_accuracy.update_state(minibatch_Y, tf.transpose(Z3))\n",
    "            \n",
    "            trainable_variables = [W1, b1, W2, b2, W3, b3]\n",
    "            grads = tape.gradient(minibatch_total_loss, trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, trainable_variables))\n",
    "            epoch_total_loss += minibatch_total_loss\n",
    "        \n",
    "        # divide the epoch total loss over the number of samples\n",
    "        epoch_total_loss /= m\n",
    "\n",
    "        # print cost every 10 epochs\n",
    "        if print_cost == True and epoch % 10 == 0:\n",
    "            print (\"Cost after epoch %i: %f\" % (epoch, epoch_total_loss))\n",
    "            print(\"Train accuracy:\", train_accuracy.result())\n",
    "            \n",
    "            # evaluate the test set every 10 epochs to avoid computational overhead\n",
    "            for (minibatch_X, minibatch_Y) in test_minibatches:\n",
    "                Z3 = forward_propagation(tf.transpose(minibatch_X), parameters)\n",
    "                test_accuracy.update_state(minibatch_Y, tf.transpose(Z3))\n",
    "            print(\"Test_accuracy:\", test_accuracy.result())\n",
    "\n",
    "            costs.append(epoch_total_loss)\n",
    "            train_acc.append(train_accuracy.result())\n",
    "            test_acc.append(test_accuracy.result())\n",
    "            test_accuracy.reset_states()\n",
    "\n",
    "    return parameters, costs, train_acc, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b7f728ba-acf2-4a15-bd4d-4145191bf14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cost(costs, lr=0.0001):\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel(\"Cost\")\n",
    "    plt.xlabel(\"iterations (per fives)\")\n",
    "    plt.title(f\"Cost (Learning rate = {lr})\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fde00bb3-f52e-49ce-b79b-171628d65ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracy(train_acc, test_acc, lr=0.0001):\n",
    "    plt.plot(np.squeeze(train_acc), label=\"Train Accuracy\")\n",
    "    plt.plot(np.squeeze(test_acc), label=\"Test Accuracy\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.xlabel(\"iterations (per fives)\")\n",
    "    plt.title(f\"Accuracy (Learning rate = {lr})\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLS",
   "language": "python",
   "name": "dls"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
