{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62d9f78c-261e-497c-8b0b-c356ff378996",
   "metadata": {},
   "source": [
    "## Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c678cd27-c30d-4401-a04b-c832c68bf3d7",
   "metadata": {},
   "source": [
    "Regularization will help you reduce overfitting by driving weights to lower values.\n",
    "\n",
    "Regularization hurts train set performance by limiting the ability of the network to overfit to the train set. But, it ultimately gives better test accuracy, it HELPS the system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73629264-bf97-49b4-9067-ef3efdcee8b9",
   "metadata": {},
   "source": [
    "## L2 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35e91a1-2ffa-4cee-b132-ba82bcdfe3bb",
   "metadata": {},
   "source": [
    "**L2 regularization** is a standard way to avoid overfitting is called.\n",
    "\n",
    "Rationale: L2-regularization relies on the assumption that a model with small weights is simpler than a model with large weights. \n",
    "1. By penalizing the square values of the weights in the cost function $J$, you drive all the weights to smaller values.\n",
    "2. The output change becomes more stable as the input changes.\n",
    "\n",
    "Remarks:\n",
    "- You can tune the value of the regularization hyperparameter $\\lambda$  using a dev set.\n",
    "- L2 regularization makes your decision boundary smoother. If $\\lambda$ is too large, it is also possible to _oversmooth_—resulting in a model with high bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa57dc8e-4d65-4d34-85f1-54405a3be272",
   "metadata": {},
   "source": [
    "#### Cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861d96d6-2ed4-4fdb-b78f-55724c189bc4",
   "metadata": {},
   "source": [
    "Modify your cost function by adding a regularization term to the cost $J$, from:\n",
    "$$J = -\\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(}\\small  y^{(i)}\\log\\left(a^{[L](i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right) \\large{)}$$\n",
    "To:\n",
    "$$J_{regularized} = \\small \\underbrace{-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(}\\small y^{(i)}\\log\\left(a^{[L](i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right) \\large{)} }_\\text{cross-entropy cost} + \\underbrace{\\frac{\\lambda}{2m} \\sum_{l=1} \\vert \\vert W^{[l]} \\vert \\vert_F^2}_\\text{L2 regularization cost}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dbab35c2-58c4-48ff-8cc5-30663802215e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    AL: probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y: true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost: cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # compute loss from aL and y.\n",
    "    logprobs = np.multiply(Y, np.log(AL)) + np.multiply((1-Y), np.log(1-AL))\n",
    "    cost = -(1/m)*np.sum(logprobs)\n",
    "    \n",
    "    cost = np.squeeze(cost) # e.g. turns [[71]] into 71\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b5e9b1e-ea03-4828-b593-b0a6ce246722",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_with_regularization(A_final, Y, parameters, lambd):\n",
    "    \"\"\"    \n",
    "    Arguments:\n",
    "    A_final: post-activation output of the final layer (prediction), shape (output size, number of examples)\n",
    "    Y: \"true\" labels vector, of shape (output size, number of examples)\n",
    "    parameters: dict containing W1, b1, ..., WL, bL\n",
    "    lambd: regularization hyperparameter\n",
    "\n",
    "    Returns:\n",
    "    cost: regularized cost\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # cross-entropy cost\n",
    "    cross_entropy_cost = compute_cost(A_final, Y)\n",
    "\n",
    "    # L2 regularization cost\n",
    "    L2_regularization_cost = 0\n",
    "    L = len(parameters) // 2  # num of layers\n",
    "    \n",
    "    for l in range(1, L + 1):\n",
    "        W = parameters[f'W{l}']\n",
    "        L2_regularization_cost += np.sum(np.square(W))\n",
    "    \n",
    "    L2_regularization_cost *= lambd / (2 * m)\n",
    "\n",
    "    # overall cost\n",
    "    cost = cross_entropy_cost + L2_regularization_cost\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcc643f-b2e3-4c44-a86c-dae092ce5f1e",
   "metadata": {},
   "source": [
    "#### Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ade5be-0797-4db2-b7ed-25791aabf0da",
   "metadata": {},
   "source": [
    "Update rule in gradient descent changes:\n",
    "$$\n",
    "\\begin{align*}\n",
    "dW^{[l]} &= \\text{(from backprop)} + \\frac{\\lambda}{m} W^{[l]} \\\\\n",
    "W^{[l]} &:= W^{[l]} - \\alpha \\cdot dW^{[l]} \\\\\n",
    "&:= W^{[l]} - \\alpha \\big[\\text{(from backprop)} + \\frac{\\lambda}{m} W^{[l]}\\big] \\\\\n",
    "&:= W^{[l]} - \\frac{\\alpha \\lambda}{m} W^{[l]} - \\alpha \\cdot \\text{(from backprop)} \\\\\n",
    "&:= \\big(1 - \\frac{\\alpha \\lambda}{m}\\big) W^{[l]} - \\alpha \\cdot \\text{(from backprop)}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$\\big(1 - \\frac{\\alpha \\lambda}{m}\\big)$ is the cause of the _weight decay_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1810514-1fcc-4ea3-bfa4-22446522178f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation_with_regularization(X, Y, caches, lambd):\n",
    "    \"\"\"\n",
    "    Implements the backward propagation with L2 regularization for an L-layer network.\n",
    "\n",
    "    Arguments:\n",
    "    X: input data, shape (input size, number of examples)\n",
    "    Y: true labels, shape (output size, number of examples)\n",
    "    caches: list of caches from forward propagation [(Z1, A0, W1, b1, A1), ..., (ZL, AL-1, WL, bL, AL)]\n",
    "    lambd: L2 regularization hyperparameter\n",
    "\n",
    "    Returns:\n",
    "    grads: dict with grads: dWl, dbl, dZl, dAl\n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    m = X.shape[1]\n",
    "    L = len(caches)\n",
    "\n",
    "    # Lth-layer\n",
    "    ZL, AL_prev, WL, bL, AL = caches[-1]\n",
    "    dZL = AL - Y\n",
    "    dWL = (1. / m) * np.dot(dZL, AL_prev.T) + (lambd / m) * WL\n",
    "    dbL = (1. / m) * np.sum(dZL, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(WL.T, dZL)\n",
    "\n",
    "    grads[f\"dZ{L}\"] = dZL\n",
    "    grads[f\"dW{L}\"] = dWL\n",
    "    grads[f\"db{L}\"] = dbL\n",
    "\n",
    "    # for L-1 hidden layers\n",
    "    for l in reversed(range(1, L)):\n",
    "        Z, A_prev, W, b, A = caches[l - 1]\n",
    "        dZ = dA_prev * (A > 0)  # ReLU derivative\n",
    "        dW = (1. / m) * np.dot(dZ, A_prev.T) + (lambd / m) * W\n",
    "        db = (1. / m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "        dA_prev = np.dot(W.T, dZ)\n",
    "\n",
    "        grads[f\"dZ{l}\"] = dZ\n",
    "        grads[f\"dW{l}\"] = dW\n",
    "        grads[f\"db{l}\"] = db\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7798716-636a-4e5a-b6a2-44af6018bdb4",
   "metadata": {},
   "source": [
    "## Dropout regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e6c2c1-e3e0-4aa7-9abf-3101e0d41a6f",
   "metadata": {},
   "source": [
    "Dropout is a regularization technique that randomly _drop_ (set to zero) some neurons in each layer with a probability of $1-\\text{keep\\_prob}$.\n",
    "\n",
    "If $\\text{keep\\_prob}=0.8$ each neuron has:\n",
    "- 80% to remain, and\n",
    "- 20% to be zeroed out.\n",
    "\n",
    "The most common implementation of dropout is the _inverted dropout_.\n",
    "\n",
    "Inverted dropout procedure:\n",
    "1. Generate a mask $d \\sim \\text{Bernoulli(keep\\_prob)}$ for each layer $l$.\n",
    "2. Apply mask using element-wise multiplication to drop units.\n",
    "3. Scale activations by dividing activations by $\\text{keep\\_prob}$.\n",
    "\n",
    "Scaling the activation via division by `keep_prob` ensures that the expected value of `a3` remains the same.\n",
    "\n",
    "Lower $\\text{keep\\_prob}$ means stronger regularization.\n",
    "\n",
    "Typical probabilities per layer:\n",
    "\n",
    "| Layer                   | keep_prob  | Remark                       |\n",
    "|-------------------------|------------|------------------------------|\n",
    "| Input                   | 0.9–1.0    | Rarely drop input features   |\n",
    "| Early hidden layers     | 0.8–0.9    | Mild regularization          |\n",
    "| Middle and deep Layers  | 0.5–0.7    | Stronger regularization      |\n",
    "| Output                  | 1.0        | No dropout for stable output |\n",
    "\n",
    "Dropout is similar to L2 as it spreads weights across inputs to reduce weight norms. However, dropout is adaptive, it:\n",
    "1. Randomly zeroes out neurons during training.\n",
    "2. Each iteration trains a smaller sub-network.\n",
    "3. Reduces reliance on specific neurons.\n",
    "4. Prevents overfitting.\n",
    "\n",
    "A downside of dropout is that the cost function $J$ becomes non-deterministic—harder to check monotic decrease. A solution for this is to first, train with dropout off to confirm convergence.\n",
    "\n",
    "Use dropout when:\n",
    "- the model overfits.\n",
    "- the model has large fully connected layers like in computer vision.\n",
    "\n",
    "\n",
    "Remarks:\n",
    "- Use dropout only in training. Don't use during test time.\n",
    "- Deep learning frameworks like TensorFlow, Keras, or PyTorch come with a dropout layer implementation.\n",
    "- Apply dropout both during forward and backward propagation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08672637-21ae-470c-aec7-09331b383474",
   "metadata": {},
   "source": [
    "#### Forward propagation with dropout\n",
    "Assumes ReLU activations for hidden layers and sigmoid for the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2cd6914b-e3a4-4e43-9d5b-85fce3b9b092",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation_with_dropout(X, parameters, keep_prob=0.5):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X: input data, shape (input size, number of examples)\n",
    "    parameters: dict containing \"W1\", \"b1\", ..., \"WL\", \"bL\"\n",
    "    keep_prob: prob of keeping a neuron active (scalar)\n",
    "    \n",
    "    Returns:\n",
    "    AL: last activation (sigmoid output)\n",
    "    caches: list of tuples for backpropagation (including dropout masks)\n",
    "    \"\"\"\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2  # num of layers\n",
    "\n",
    "    for l in range(1, L):\n",
    "        W = parameters[f'W{l}']\n",
    "        b = parameters[f'b{l}']\n",
    "        Z = np.dot(W, A) + b\n",
    "        A = relu(Z)\n",
    "\n",
    "        D = np.random.rand(A.shape[0], A.shape[1]) < keep_prob  # dropout mask\n",
    "        A = np.multiply(A, D) # apply mask\n",
    "        A /= keep_prob # scale the activation\n",
    "\n",
    "        caches.append((Z, D, A, W, b))\n",
    "\n",
    "    # final layer (no dropout, sigmoid activation)\n",
    "    WL = parameters[f'W{L}']\n",
    "    bL = parameters[f'b{L}']\n",
    "    ZL = np.dot(WL, A) + bL\n",
    "    AL = sigmoid(ZL)\n",
    "\n",
    "    caches.append((ZL, None, AL, WL, bL))  # no dropout in output\n",
    "\n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbb46e6-7b58-408b-9b91-33802ca4e173",
   "metadata": {},
   "source": [
    "#### Backward propagation with dropout\n",
    "Implements the backward propagation for an L-layer neural network with dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d2a7d45-3f4e-47ba-bfdd-2aa6bb438d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation_with_dropout(X, Y, caches, keep_prob):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X: input data, shape (input size, number of examples)\n",
    "    Y: true labels, shape (output size, number of examples)\n",
    "    caches: list of tuples from forward_propagation_with_dropout\n",
    "            (Z, D, A, W, b) for hidden layers, (Z, None, A, W, b) for output layer\n",
    "    keep_prob: dropout keep probability (scalar)\n",
    "\n",
    "    Returns:\n",
    "    grads: dict with grads\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    L = len(caches)\n",
    "    grads = {}\n",
    "\n",
    "    # output layer (sigmoid, no dropout)\n",
    "    ZL, _, AL, WL, bL = caches[-1]\n",
    "    dZL = AL - Y\n",
    "    grads[f'dW{L}'] = (1 / m) * np.dot(dZL, caches[-2][2].T if L > 1 else X.T)\n",
    "    grads[f'db{L}'] = (1 / m) * np.sum(dZL, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(WL.T, dZL)\n",
    "\n",
    "    # loop through hidden layers (ReLU + dropout), in reverse\n",
    "    for l in reversed(range(1, L)):\n",
    "        Z, D, A, W, b = caches[l - 1]\n",
    "        dA_prev = np.multiply(dA_prev, D) # apply dropout mask\n",
    "        dA_prev /= keep_prob              # scale\n",
    "        dZ = dA_prev * (Z > 0)            # ReLU derivative\n",
    "        A_prev = caches[l - 2][2] if l > 1 else X\n",
    "        grads[f'dW{l}'] = (1 / m) * np.dot(dZ, A_prev.T)\n",
    "        grads[f'db{l}'] = (1 / m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "        dA_prev = np.dot(W.T, dZ)\n",
    "\n",
    "    return grads"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLS",
   "language": "python",
   "name": "dls"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
