{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e640ff9-5194-4d54-ac2b-ef22bcd0229e",
   "metadata": {},
   "source": [
    "# Gradient checking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246effd1-e06b-47eb-a69a-bc26011219b8",
   "metadata": {},
   "source": [
    "Use _gradient checking_ to verify correctness when implementing backpropagation.\n",
    "\n",
    "Gradient checking relies on the numerical approximation of gradients.\n",
    "\n",
    "Remarks:\n",
    "- Don't use during training. Use only to debug.\n",
    "- If grad check doesn't match, inspect which gradient component diverges.\n",
    "- If regularizing, include regularization term when calculating both $d\\theta_\\text{approx}$ and $d\\theta$.\n",
    "- Grad check doesn't work with dropout. Implement grad check first, then re-enable dropout using `keep_prob = 1.0`.\n",
    "- It is not impossible that your backprop is only correct when it is near initialization. Run grad check after at random initialization and after a few training steps to confirm correctness. This is seldomly done.\n",
    "- Gradient checking is slow, run it only to make sure your code is correct. Turn it off and use backprop for the actual learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba0df4a-881b-4459-a828-baab8c02ac8f",
   "metadata": {},
   "source": [
    "## 1-D grad check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9049452-17ce-445e-91ae-4b74aa3141c3",
   "metadata": {},
   "source": [
    "Definition of a derivative (or gradient):$$ \\frac{\\partial J}{\\partial \\theta} = \\lim_{\\varepsilon \\to 0} \\frac{J(\\theta + \\varepsilon) - J(\\theta - \\varepsilon)}{2 \\varepsilon}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04eebe99-2c21-410e-bc90-4a875a49af1b",
   "metadata": {},
   "source": [
    "#### 1-D forward prop\n",
    "Implement the linear forward propagation (J(theta) = theta * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3e71d13-c602-40a6-9d47-61f249dad4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(x, theta):\n",
    "    \"\"\"    \n",
    "    Arguments:\n",
    "    x: a real-valued input\n",
    "    theta: our parameter, a real number as well\n",
    "    \n",
    "    Returns:\n",
    "    J: the value of function J, computed using the formula J(theta) = theta * x\n",
    "    \"\"\"\n",
    "    \n",
    "    J = theta * x\n",
    "    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcf6810-d7e0-4b26-b75c-1bf81e0c8e2d",
   "metadata": {},
   "source": [
    "#### 1-D backward prop\n",
    "Computes the derivative of J with respect to theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "063fbddd-06cd-41f1-b252-873ebe658c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(x, theta):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    x: a real-valued input\n",
    "    theta: our param, a real number as well\n",
    "    \n",
    "    Returns:\n",
    "    dthetaâ€¦ the grad of the cost wrt theta\n",
    "    \"\"\"\n",
    "    \n",
    "    dtheta = x\n",
    "    \n",
    "    return dtheta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffb12d9-3f52-420f-9a66-7d906d740487",
   "metadata": {},
   "source": [
    "#### 1-D gradcheck function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e43bda-6f0f-49b5-b221-fdfc7cb904b8",
   "metadata": {},
   "source": [
    "Implement gradcheck to show that `backward_propagation()` is correctly computing the grad gradient $\\frac{\\partial J}{\\partial \\theta}$.\n",
    "\n",
    "Grad check procedure:\n",
    "1. Compute gradient approximation:\n",
    "    1. $\\theta^{+} = \\theta + \\varepsilon$\n",
    "    2. $\\theta^{-} = \\theta - \\varepsilon$\n",
    "    3. $J^{+} = J(\\theta^{+})$\n",
    "    4. $J^{-} = J(\\theta^{-})$\n",
    "    5. $\\text{gradapprox} = \\frac{J^{+} - J^{-}}{2  \\varepsilon}$\n",
    "2. Compute the gradient using backward propagation, and store the result in a variable \"grad\".\n",
    "3. Compute the relative difference between \"gradapprox\" and the \"grad\":\n",
    "$$\\text{difference} = \\frac {\\mid\\mid \\text{grad} - \\text{gradapprox} \\mid\\mid_2}{\\mid\\mid \\text{grad} \\mid\\mid_2 + \\mid\\mid \\text{gradapprox} \\mid\\mid_2}$$\n",
    "4. Check difference\n",
    "   - $\\text{difference} \\approx 10^{-7}$: likely correct \n",
    "   - $\\text{difference} \\approx 10^{-5}$: okay but double-check\n",
    "   - $\\text{difference} \\approx 10^{-3}$: worry, bug in backprop\n",
    "\n",
    "How to compute for step 3?\n",
    "1.  compute the numerator using np.linalg.norm(...)\n",
    "2. compute the denominator. You will need to call np.linalg.norm(...) twice.\n",
    "3. divide them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30994913-c495-444a-a472-194d00c6d156",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check(x, theta, epsilon=1e-7, print_msg=False):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    x: a float input\n",
    "    theta: our parameter, a float as well\n",
    "    epsilon: tiny shift to the input to compute approx grad\n",
    "    \n",
    "    Returns:\n",
    "    difference: difference (2) between the approximated gradient and the backward propagation gradient -> float\n",
    "    \"\"\"\n",
    "    \n",
    "    # compute gradapprox\n",
    "    theta_plus = theta + epsilon\n",
    "    theta_minus = theta - epsilon\n",
    "    J_plus = forward_propagation(x, theta_plus)\n",
    "    J_minus = forward_propagation(x, theta_minus)\n",
    "    gradapprox = (J_plus - J_minus)/(2*epsilon)\n",
    "\n",
    "    \n",
    "    # compute grad using backprop\n",
    "    grad = backward_propagation(x, theta)\n",
    "\n",
    "    # compute difference\n",
    "    numerator = np.linalg.norm(grad-gradapprox)\n",
    "    denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox)\n",
    "    difference = numerator/denominator\n",
    "    \n",
    "    if print_msg:\n",
    "        if difference > 2e-7:\n",
    "            print (\"\\033[93m\" + \"There is a mistake in the backward propagation! difference = \" + str(difference) + \"\\033[0m\")\n",
    "        else:\n",
    "            print (\"\\033[92m\" + \"Your backward propagation works perfectly fine! difference = \" + str(difference) + \"\\033[0m\")\n",
    "    \n",
    "    return difference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6148a5-3b92-4fda-9d20-31d6d1595fa5",
   "metadata": {},
   "source": [
    "#### L-layer N-D forward prop\n",
    "Implements forward propagation for an L-layer NN with ReLU activations and a sigmoid output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "941de6d4-7e37-4d7f-85e7-4fae9c661596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    AL: probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y: true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost: cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # compute loss from aL and y.\n",
    "    logprobs = np.multiply(Y, np.log(AL)) + np.multiply((1-Y), np.log(1-AL))\n",
    "    cost = -(1/m)*np.sum(logprobs)\n",
    "    \n",
    "    cost = np.squeeze(cost) # e.g. turns [[71]] into 71\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c60c368-02e4-4425-bd33-e3bc7aa18add",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation_L(X, parameters):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X: input data, shape (input size, number of examples)\n",
    "    parameters: dict with W1, b1, ..., WL, bL\n",
    "\n",
    "    Returns:\n",
    "    AL: last activation value\n",
    "    caches: list of caches: (Z, A_prev, W, b)\n",
    "    \"\"\"\n",
    "    \n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2  # num of layers\n",
    "\n",
    "    # hidden layers: LINEAR -> RELU\n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        W = parameters[f'W{l}']\n",
    "        b = parameters[f'b{l}']\n",
    "        Z = np.dot(W, A_prev) + b\n",
    "        A = relu(Z)\n",
    "        caches.append((Z, A_prev, W, b))\n",
    "\n",
    "    # output layer: LINEAR -> SIGMOID\n",
    "    WL = parameters[f'W{L}']\n",
    "    bL = parameters[f'b{L}']\n",
    "    ZL = np.dot(WL, A) + bL\n",
    "    AL = sigmoid(ZL)\n",
    "    caches.append((ZL, A, WL, bL))\n",
    "\n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf728995-b1be-4f5e-9d66-5ba0b2c898d8",
   "metadata": {},
   "source": [
    "#### L-layer N-D backward prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97cfa4f6-fb2f-4523-9d0f-ab63a7fed8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_derivative(Z):\n",
    "    return Z > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b01e224a-89f6-436d-9688-1bc331f0a373",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_derivative(Z):\n",
    "    s = 1 / (1 + np.exp(-Z))\n",
    "    return s * (1 - s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b38016b6-f3a1-49a7-95e6-74a04b60ec49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation_L(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implements backward propagation for an L-layer NN.\n",
    "    \n",
    "    Arguments:\n",
    "    AL: probability vector, output of the forward propagation\n",
    "    Y: true \"label\" vector (same shape as AL)\n",
    "    caches: list of caches from forward_propagation_L:\n",
    "              each cache is (Z, A_prev, W, b)\n",
    "\n",
    "    Returns:\n",
    "    grads: dict with the grads dW, db, dA for each layer\n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches)\n",
    "    m = Y.shape[1]\n",
    "    Y = Y.reshape(AL.shape)\n",
    "\n",
    "    # init gradient from output layer (sigmoid)\n",
    "    dAL = -(np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "\n",
    "    # output layer cache\n",
    "    current_cache = caches[L-1]\n",
    "    ZL, A_prev, WL, bL = current_cache\n",
    "    dZL = dAL * sigmoid_derivative(ZL)\n",
    "    grads[f'dW{L}'] = (1/m) * np.dot(dZL, A_prev.T)\n",
    "    grads[f'db{L}'] = (1/m) * np.sum(dZL, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(WL.T, dZL)\n",
    "\n",
    "    # loop for L-1 to 1\n",
    "    for l in reversed(range(L - 1)):\n",
    "        Z, A_prev, W, b = caches[l]\n",
    "        dZ = dA_prev * relu_derivative(Z)\n",
    "        grads[f'dW{l+1}'] = (1/m) * np.dot(dZ, A_prev.T)\n",
    "        grads[f'db{l+1}'] = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "        dA_prev = np.dot(W.T, dZ)\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308d0ee7-a008-4046-8a76-50cf3cddddde",
   "metadata": {},
   "source": [
    "#### L-layer N-D grad check function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801b26ac-2480-470b-b18b-e844b1aa9569",
   "metadata": {},
   "source": [
    "For each $i$ in num_parameters:\n",
    "1. Compute `J_plus[i]`:\n",
    "    1. Set $\\theta^{+}$ to `np.copy(parameters_values)`\n",
    "    2. Set $\\theta^{+}_i$ to $\\theta^{+}_i + \\varepsilon$\n",
    "    3. Calculate $J^{+}_i$ using to `forward_propagation_n(x, y, vector_to_dictionary(`$\\theta^{+}$ `))`.     \n",
    "2. Compute `J_minus[i]`: do the same thing with $\\theta^{-}$\n",
    "3. Compute $gradapprox[i] = \\frac{J^{+}_i - J^{-}_i}{2 \\varepsilon}$\n",
    "4. Compute the relative difference between \"gradapprox\" and the \"grad\":\n",
    "$$\\text{difference} = \\frac {\\mid\\mid \\text{grad} - \\text{gradapprox} \\mid\\mid_2}{\\mid\\mid \\text{grad} \\mid\\mid_2 + \\mid\\mid \\text{gradapprox} \\mid\\mid_2}$$\n",
    "5. Check difference\n",
    "   - $\\text{difference} \\approx 10^{-7}$: likely correct \n",
    "   - $\\text{difference} \\approx 10^{-5}$: okay but double-check\n",
    "   - $\\text{difference} \\approx 10^{-3}$: worry, bug in backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0019682-ad31-4350-90bf-5c58f076f1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check_n(parameters, gradients, X, Y, epsilon=1e-7, print_msg=False):\n",
    "    \"\"\"\n",
    "    Checks if backward_propagation_n computes correct gradients using numerical approximation.\n",
    "    \n",
    "    Arguments:\n",
    "    parameters: dict of your L-layer neural network parameters (W1, b1, ..., WL, bL)\n",
    "    gradients: output of backward_propagation_n, same structure as parameters\n",
    "    X: input data (input size, number of examples)\n",
    "    Y: true labels\n",
    "    epsilon: small scalar used for finite differences\n",
    "    \n",
    "    Returns:\n",
    "    difference: rel. diff. between analytical and numerical gradients\n",
    "    \"\"\"\n",
    "\n",
    "    # flatten params and grads\n",
    "    parameters_vector, _ = dictionary_to_vector(parameters)\n",
    "    grad_vector = gradients_to_vector(gradients)\n",
    "    \n",
    "    num_parameters = parameters_vector.shape[0]\n",
    "    J_plus = np.zeros((num_parameters, 1))\n",
    "    J_minus = np.zeros((num_parameters, 1))\n",
    "    gradapprox = np.zeros((num_parameters, 1))\n",
    "    \n",
    "    # compute numerical gradient approx\n",
    "    for i in range(num_parameters):\n",
    "        theta_plus = np.copy(parameters_vector)\n",
    "        theta_plus[i][0] += epsilon\n",
    "        J_plus[i], _ = forward_propagation_n(X, Y, vector_to_dictionary(theta_plus))\n",
    "\n",
    "        theta_minus = np.copy(parameters_vector)\n",
    "        theta_minus[i][0] -= epsilon\n",
    "        J_minus[i], _ = forward_propagation_n(X, Y, vector_to_dictionary(theta_minus))\n",
    "        \n",
    "        gradapprox[i] = (J_plus[i] - J_minus[i]) / (2 * epsilon)\n",
    "    \n",
    "    # compute difference\n",
    "    numerator = np.linalg.norm(grad_vector - gradapprox)\n",
    "    denominator = np.linalg.norm(grad_vector) + np.linalg.norm(gradapprox)\n",
    "    difference = numerator / denominator\n",
    "\n",
    "    if print_msg:\n",
    "        if difference > 2e-7:\n",
    "            print(f\"\\033[93mWarning: gradient check failed! Difference = {difference}\\033[0m\")\n",
    "        else:\n",
    "            print(f\"\\033[92mGradient check passed! Difference = {difference}\\033[0m\")\n",
    "\n",
    "    return difference"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLS",
   "language": "python",
   "name": "dls"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
