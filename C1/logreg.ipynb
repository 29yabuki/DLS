{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99983218-e31e-4adf-8be3-cb9e279c4fe4",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031c7596-98fd-4783-a9e6-38d4f05aeb17",
   "metadata": {},
   "source": [
    "Logistic regression estimates the probability that an input belongs to class 1 using a sigmoid function and is trained by minimizing a loss over all examples.\n",
    "\n",
    "For a single example $x^{(i)}$:\n",
    "$$z^{(i)} = w^T x^{(i)} + b \\tag{1}$$\n",
    "$$\\hat{y}^{(i)} = a^{(i)} = \\text{sigmoid}(z^{(i)})\\tag{2}$$ \n",
    "$$ \\mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(a^{(i)}) - (1-y^{(i)} )  \\log(1-a^{(i)})\\tag{3}$$\n",
    "\n",
    "The cost is then computed by summing over all training examples:\n",
    "$$ J = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(a^{(i)}, y^{(i)})\\tag{4}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45611cd0-deed-40a3-9d23-124a80dd5ac1",
   "metadata": {},
   "source": [
    "## 00 Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbea7cbc-a1b0-41c8-b453-480bdd26f0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82967538-f2b5-4f90-88cc-444ea5c9a97c",
   "metadata": {},
   "source": [
    "## 01 Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a957ab0-a700-4eb5-a8bc-8bd4c5f514d9",
   "metadata": {},
   "source": [
    "#### 1. Determine key dimensions:\n",
    "- m_train (number of training examples)\n",
    "- m_test (number of test examples)\n",
    "\n",
    "If image:\n",
    "- num_px (= height = width of a training image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7651ae0-f556-468b-a5c5-ae8645dc69a0",
   "metadata": {},
   "source": [
    "#### 2. Reshape the data\n",
    "\n",
    "If image, flatten it into the following shape $(\\text{num\\_px} \\cdot \\text{num\\_px} \\cdot 3, 1)$ using the `img2vec` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbf1fb88-eeef-4628-aae1-948d87102d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image2vector(image):\n",
    "    return image.reshape(image.shape[0]*image.shape[1]*image.shape[2], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca041b7d-6828-40d7-a83a-d265256b3e4e",
   "metadata": {},
   "source": [
    "When flattening a matrix $X$ of shape (a, b, c, d) to (b * c * d, a), use:\n",
    "```python\n",
    "X_flatten = X.reshape(X.shape[0], -1).T\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319cc51b-8b89-4196-b37a-f3688f2a9bfd",
   "metadata": {},
   "source": [
    "#### 3. Standardize the data\n",
    "\n",
    "Standardizing data prevents large features from dominating the learning—leading to faster convergence, better stability, and improved model performance.\n",
    "\n",
    "Below are ways of normalizing data. For images, only pixel value normalization is used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a33dc5-eb8c-4739-99ce-90d9e6d8b657",
   "metadata": {},
   "source": [
    "##### pixel value normalization\n",
    "Each pixel in a color image has 3 values (RGB), each from 0 to 255. \n",
    "\n",
    "It's common to normalize by dividing by 255.\n",
    "\n",
    "This scales pixel values to the [0, 1] range for easier model training.\n",
    "```python\n",
    "train_set_x = train_set_x_flatten/255\n",
    "test_set_x = test_set_x_flatten/255\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b214078a-1fca-4bb4-aa91-33e260329ea6",
   "metadata": {},
   "source": [
    "##### row normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fcc038-c46c-4d79-b801-f6ea4c4ced7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_rows(x):\n",
    "    x_norm = np.linalg.norm(x, axis=1, keepdims=True)\n",
    "    x /= x_norm\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c7007f-13c5-4c9a-89f7-a62c49cdd362",
   "metadata": {},
   "source": [
    "## 02 Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4afbba-e5ac-4e90-96eb-b7a6ef18033a",
   "metadata": {},
   "source": [
    "### Sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0425ef-561d-4480-960c-a5da6d003d4e",
   "metadata": {},
   "source": [
    "$$\\text{sigmoid}(z) = \\sigma(z) = \\frac{1}{1 + e^{-z}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc1604a4-1a44-4749-b7ab-f88194ffcf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9238a4d8-ac1c-49b9-bc1b-35c3ee3ceea5",
   "metadata": {},
   "source": [
    "### Sigmoid derivative"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f5968163-21bf-492d-96ed-da5d18dcd9ec",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{d}{dz} \\sigma(z) = \\sigma(z)\\left(1-\\sigma(z)\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ce20592-e015-4aa4-bb7a-70632bd39b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_derivative(z):\n",
    "    s = sigmoid(z)\n",
    "    return s * (1 - s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e76f4db-3438-4a2f-a11b-87ad615893f4",
   "metadata": {},
   "source": [
    "### Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6066b933-3ee4-46b0-a694-d23f2cf08233",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49dce791-39f2-472c-ba8f-460d949afd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    x_exp = np.exp(x)\n",
    "    x_sum = np.sum(x_exp, axis=1, keepdims=True)\n",
    "    s = x_exp/x_sum\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c17a8e-eaa7-466c-a633-884c64975cac",
   "metadata": {},
   "source": [
    "## 03 Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43107ba0-1ceb-4275-9f97-dfd0b7ea3e43",
   "metadata": {},
   "source": [
    "Initialize a parameters as follows:\n",
    "- $w$ as a vector of zeros.\n",
    "- $b$ to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec0f2858-e11f-41aa-b714-b9cfb48971cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_with_zeros(dim):\n",
    "    w = np.zeros((dim, 1))\n",
    "    b = float(0)\n",
    "\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61339c91-57f8-48db-bbbe-5e8ee7c51c88",
   "metadata": {},
   "source": [
    "## 04 Forward and backward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d70248f-bb14-4a64-a81e-71834e1734cc",
   "metadata": {},
   "source": [
    "With parameters initialized:\n",
    "1. Perform forward propagation to compute predictions and the cost.\n",
    "2. Perform backward propagation to compute gradients for learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dea5fca-979c-4078-8583-5d32218e189e",
   "metadata": {},
   "source": [
    "#### Forward propagation\n",
    "1. Get $X$\n",
    "2. Compute\n",
    "$$A = \\sigma(w^T X + b) = (a^{(1)}, a^{(2)}, ..., a^{(m-1)}, a^{(m)}) \\tag{5}$$\n",
    "3. Compute the cost function:\n",
    "$$J = -\\frac{1}{m}\\sum_{i=1}^{m}(y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})) \\tag{6}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ec53c5-0e47-4674-baa8-bf33db43adbd",
   "metadata": {},
   "source": [
    "#### Backward propogation\n",
    "Compute for the following gradients:\n",
    "$$dw = \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T\\tag{7}$$\n",
    "$$db = \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})\\tag{8}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966dcd87-bc67-49e7-801e-59fda7510471",
   "metadata": {},
   "source": [
    "The `propagate()` computes the cost function and its gradient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a053096-ced6-425e-9385-6fc1a18733f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate(w, b, X, Y):\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    A = sigmoid(np.dot(w.T, X) + b)\n",
    "    cost = -(1/m)*(np.dot(Y, np.log(A).T) + np.dot((1-Y), np.log(1-A).T))\n",
    "    \n",
    "    dw = (1/m)*np.dot(X, (A-Y).T)\n",
    "    db = (1/m)*np.sum(A-Y)\n",
    "    \n",
    "    cost = np.squeeze(np.array(cost))\n",
    "\n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return grads, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39630ecb-1bad-452a-8037-f7936ceb2850",
   "metadata": {},
   "source": [
    "## 05 Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e958744-354e-420a-a3ad-bc45fd6ccd8b",
   "metadata": {},
   "source": [
    "When parameters are:\n",
    "- Initialized\n",
    "- The cost and gradients are computed.\n",
    "\n",
    "The next step is to do optimization.\n",
    "\n",
    "Optimization is the use of gradients from backprop to:\n",
    "- update the parameters, and\n",
    "- reduce the cost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d1331b-24ab-4db0-a89f-781cc61946d5",
   "metadata": {},
   "source": [
    "The `optimization` function guides the learning of $w$ and $b$ by minimizing the cost $J$.\n",
    "\n",
    "Each parameter $\\theta$ is updated using $\\theta = \\theta - \\alpha \\, d\\theta$, where $\\alpha$ is the learning rate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26b8e549-8400-4e34-9a98-777c70791c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(w, b, X, Y, num_iterations=100, learning_rate=0.009, print_cost=False):    \n",
    "    w = copy.deepcopy(w)\n",
    "    b = copy.deepcopy(b)\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        # Cost and gradient calculation \n",
    "        grads, cost = propagate(w, b, X, Y)\n",
    "        \n",
    "        # Retrieve derivatives from grads\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "        \n",
    "        # update rule (≈ 2 lines of code)\n",
    "        w = w - learning_rate*dw\n",
    "        b = b - learning_rate*db\n",
    "        \n",
    "        # Record the costs\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        \n",
    "            # Print the cost every 100 training iterations\n",
    "            if print_cost:\n",
    "                print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return params, grads, costs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cfb26e-d544-4b21-90c2-85e18cebc700",
   "metadata": {},
   "source": [
    "## 06 Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256f4615-4570-4b85-ab3c-8fe9b2a141c0",
   "metadata": {},
   "source": [
    "The `predict(w, b, X)` function uses the logistic regression parameters $w$ and $b$ to compute predictions (0 or 1) for each example in $X$.\n",
    "\n",
    "It applies the sigmoid function $\\sigma(z)$ and thresholding at 0.5. \n",
    "\n",
    "It returns a NumPy array `Y_prediction` containing these binary predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "971d1031-62fb-4789-866f-e7efa4bd34fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w, b, X):    \n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1, m))\n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "    A = sigmoid(np.dot(w.T, X) + b)\n",
    "    \n",
    "    for i in range(A.shape[1]):\n",
    "        if A[0, i] > 0.5:\n",
    "            Y_prediction[0, i] = 1\n",
    "        else:\n",
    "            Y_prediction[0, i] = 0\n",
    "    \n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447f3bd5-8aa0-4497-aa97-912062623d2f",
   "metadata": {},
   "source": [
    "## 07 Merge into a model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d53ec33-4ece-4694-a987-dbfaee8db1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, num_iterations=2000, learning_rate=0.5, print_cost=False):\n",
    "    w, b = initialize_with_zeros(X_train.shape[0])\n",
    "    \n",
    "    # Gradient descent\n",
    "    params, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n",
    "\n",
    "    # Get parameters w and b from dictionary \"params\"\n",
    "    w = params['w']\n",
    "    b = params['b']\n",
    "\n",
    "    # Predict test/train set examples\n",
    "    Y_prediction_test = predict(w, b, X_test)\n",
    "    Y_prediction_train = predict(w, b, X_train)\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "\n",
    "    # Print train/test Errors\n",
    "    if print_cost:\n",
    "        print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
    "        print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "\n",
    "    \n",
    "    d = {\"costs\": costs,\n",
    "         \"Y_prediction_test\": Y_prediction_test, \n",
    "         \"Y_prediction_train\" : Y_prediction_train, \n",
    "         \"w\" : w, \n",
    "         \"b\" : b,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd901db1-168f-490c-9820-05abd4698e86",
   "metadata": {},
   "source": [
    "## 08 Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb35631b-f103-4d40-86b4-6e6d852dc5be",
   "metadata": {},
   "source": [
    "Run the following cell to train your model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa14f02-d9a5-427d-baf1-23c99242dbee",
   "metadata": {},
   "source": [
    "````python\n",
    "logistic_regression_model = model(train_set_x,\n",
    "                                  train_set_y,\n",
    "                                  test_set_x,\n",
    "                                  test_set_y,\n",
    "                                  num_iterations=2000, \n",
    "                                  learning_rate=0.005, \n",
    "                                  print_cost=True)\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea32552d-177f-4105-9c6f-6dcc7bd7a700",
   "metadata": {},
   "source": [
    "## 09 Data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966291eb-b555-4c49-8ec2-8027c2443ef5",
   "metadata": {},
   "source": [
    "### Plot learning curve (with costs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ed3ca3-72e8-496e-908b-81003db2ec4b",
   "metadata": {},
   "source": [
    "```python\n",
    "costs = np.squeeze(logistic_regression_model['costs'])\n",
    "plt.plot(costs)\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations (per hundreds)')\n",
    "plt.title(\"Learning rate =\" + str(logistic_regression_model[\"learning_rate\"]))\n",
    "plt.show()\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e65a886-45b7-4dae-ac9f-5da3d5d37818",
   "metadata": {},
   "source": [
    "## 10 Additional remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1209f5-5e76-4794-8ba9-0b38af0e6e8c",
   "metadata": {},
   "source": [
    "### Learning rate\n",
    "\n",
    "Learning rate is a hyperparameter.\n",
    "\n",
    "The learning rate $\\alpha$ controls how quickly gradient descent updates the parameters. \n",
    "- If it's too large, the model may overshoot the minimum.\n",
    "- If too small, convergence will be very slow.\n",
    "\n",
    "Tuning the learning rate can significantly impact the convergence speed and performance of the algorithm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
