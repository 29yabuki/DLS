{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f2922b9-201b-4d9c-a5d6-c25421a2abe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17240348-dc4b-4d6f-a53b-40e700463f90",
   "metadata": {},
   "source": [
    "## 01 Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb558fc-d7a1-4142-98f3-be5a94477382",
   "metadata": {},
   "source": [
    "### 01.01 Explore the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b26b5e-87ad-4d3d-8442-18abaa85d778",
   "metadata": {},
   "source": [
    "```python\n",
    "m_train = train_x_orig.shape[0]\n",
    "num_px = train_x_orig.shape[1]\n",
    "m_test = test_x_orig.shape[0]\n",
    "\n",
    "print(\"Number of training examples: \" + str(m_train))\n",
    "print(\"Number of testing examples: \" + str(m_test))\n",
    "print(\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n",
    "print(\"train_x_orig shape: \" + str(train_x_orig.shape))\n",
    "print(\"train_y shape: \" + str(train_y.shape))\n",
    "print(\"test_x_orig shape: \" + str(test_x_orig.shape))\n",
    "print(\"test_y shape: \" + str(test_y.shape))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a959501-b893-48ca-bdee-773ec7251b1a",
   "metadata": {},
   "source": [
    "### 01.02 Reshaping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2fbd25-1481-42d6-b9ea-b0d10e4607ae",
   "metadata": {},
   "source": [
    "```python\n",
    "train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T # -1 flatten the remaining dims\n",
    "test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a63ef57-d7d2-4617-b062-6f9f76ee7ef4",
   "metadata": {},
   "source": [
    "### 01.03 Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec4a647-7418-4b03-9db3-4101f427a8c7",
   "metadata": {},
   "source": [
    "```python\n",
    "train_x = train_x_flatten/255\n",
    "test_x = test_x_flatten/255\n",
    "\n",
    "print(\"train_x's shape: \" + str(train_x.shape))\n",
    "print(\"test_x's shape: \" + str(test_x.shape))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e38e55-23f0-43d1-81c8-81186286fb9d",
   "metadata": {},
   "source": [
    "## 02 Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8d5fe3-b72b-43f9-80b4-94d87c3433e3",
   "metadata": {},
   "source": [
    "### 02.01 2-layer NN\n",
    "Implementation of a two-layer neural network: LINEAR->RELU->LINEAR->SIGMOID."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40521049-c196-4dc5-9092-bba624a825a4",
   "metadata": {},
   "source": [
    "#### 02.01.01 Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "696c7a38-04a6-45c7-84e0-fe6d00ca992a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_x = 12288     # num_px * num_px * 3\n",
    "n_h = 7\n",
    "n_y = 1\n",
    "layers_dims = (n_x, n_h, n_y)\n",
    "learning_rate = 0.0075"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6c1978-b65b-4dbe-8391-1b58d0bb5c3a",
   "metadata": {},
   "source": [
    "#### 02.01.02 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5287099b-e393-4604-b524-c9fdb5651570",
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X: input data, of shape (n_x, number of examples)\n",
    "    Y:- true \"label\" vector (containing 1 if cat, 0 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims: dims of the layers (n_x, n_h, n_y)\n",
    "    num_iterationsL num of iterations of the optimization loop\n",
    "    learning_rate: learning rate of the gradient descent update rule\n",
    "    print_cost: If True, this will print the cost every 100 iterations \n",
    "    \n",
    "    Returns:\n",
    "    parameters: dict containing W1, W2, b1, and b2\n",
    "    \"\"\"\n",
    "    \n",
    "    # np.random.seed(1)\n",
    "    grads = {}\n",
    "    costs = []                               # keep track of the cost\n",
    "    m = X.shape[1]                           # number of examples\n",
    "    (n_x, n_h, n_y) = layers_dims\n",
    "    \n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    # gradient descent loop\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward prop: LINEAR -> RELU -> LINEAR -> SIGMOID\n",
    "        A1, cache1 = linear_activation_forward(X, W1, b1, 'relu')\n",
    "        A2, cache2 = linear_activation_forward(A1, W2, b2, 'sigmoid')\n",
    "        \n",
    "        # Compute cost\n",
    "        cost = compute_cost(A2, Y)\n",
    "        \n",
    "        # Initializing back prop\n",
    "        dA2 = - (np.divide(Y, A2) - np.divide(1 - Y, 1 - A2))\n",
    "        \n",
    "        # Back prop\n",
    "        dA1, dW2, db2 = linear_activation_backward(dA2, cache2, 'sigmoid')\n",
    "        dA0, dW1, db1 = linear_activation_backward(dA1, cache1, 'relu')\n",
    "            \n",
    "        grads['dW1'] = dW1\n",
    "        grads['db1'] = db1\n",
    "        grads['dW2'] = dW2\n",
    "        grads['db2'] = db2\n",
    "        \n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        W1 = parameters[\"W1\"]\n",
    "        b1 = parameters[\"b1\"]\n",
    "        W2 = parameters[\"W2\"]\n",
    "        b2 = parameters[\"b2\"]\n",
    "        \n",
    "        if print_cost and (i % 100 == 0 or i == num_iterations - 1):\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    return parameters, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72c50856-6ef5-4ee6-9056-08a407ba1533",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_costs(costs, learning_rate=0.0075):\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66630ba-7a74-4462-96fe-9f735d26bdd3",
   "metadata": {},
   "source": [
    "#### 02.01.03 Compute cost\n",
    "```python\n",
    "parameters, costs = two_layer_model(train_x, train_y, layers_dims = (n_x, n_h, n_y), num_iterations = 2, print_cost=False)\n",
    "\n",
    "print(\"Cost after first iteration: \" + str(costs[0]))\n",
    "\n",
    "two_layer_model_test(two_layer_model)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ac8c96-48f4-4731-a9e8-91ca6a543d89",
   "metadata": {},
   "source": [
    "#### 02.01.04 Training\n",
    "```python\n",
    "parameters, costs = two_layer_model(train_x, train_y, layers_dims = (n_x, n_h, n_y), num_iterations = 2500, print_cost=True)\n",
    "plot_costs(costs, learning_rate)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe365ea-0e8e-4d05-b2e6-fb27ec157eb8",
   "metadata": {},
   "source": [
    "#### 02.01.04 Predictions\n",
    "```python\n",
    "predictions_train = predict(train_x, train_y, parameters) # predict on train data\n",
    "predictions_test = predict(test_x, test_y, parameters) # predict on test data\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7957314-8134-4efb-9822-74b4697bf92b",
   "metadata": {},
   "source": [
    "### 02.02 L-layer model\n",
    "Builds a $L$-layer neural network of the ff. structure: [LINEAR -> RELU]$\\times$(L-1) -> LINEAR -> SIGMOID."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c708976-461c-41e0-9782-220e1b47f274",
   "metadata": {},
   "source": [
    "#### 02.02.01 Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e47c126-f1b0-4a81-a8aa-0bf1057f4f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_dims = [12288, 20, 7, 5, 1] #  4-layer model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312f7209-208c-43eb-8383-9129e5032721",
   "metadata": {},
   "source": [
    "#### 02.02.02 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "072c7aa5-31fa-488c-a81d-198ea129fa55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L_layer_model\n",
    "\n",
    "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X: input data, of shape (n_x, number of examples)\n",
    "    Y : true \"label\" vector (containing 1 if cat, 0 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims: list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate: learning rate of the gradient descent update rule\n",
    "    num_iterations: num of iterations of the optimization loop\n",
    "    print_cost: if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters: params learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    # np.random.seed(1)\n",
    "    costs = []                         # keep track of cost\n",
    "    \n",
    "    # Parameters initialization\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    \n",
    "    # Gradient descent loop\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "        \n",
    "        # Compute cost\n",
    "        cost = compute_cost(AL, Y)\n",
    "        \n",
    "        # Back prop\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    "\n",
    "        # Update parameters\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "                \n",
    "        # Print the cost every 100 iterations and for the last iteration\n",
    "        if print_cost and (i % 100 == 0 or i == num_iterations - 1):\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "    \n",
    "    return parameters, costs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b375b30e-8516-4c67-b905-eeb34b806f8f",
   "metadata": {},
   "source": [
    "#### 02.02.03 Compute cost\n",
    "```python\n",
    "parameters, costs = L_layer_model(train_x, train_y, layers_dims, num_iterations = 1, print_cost = False)\n",
    "\n",
    "print(\"Cost after first iteration: \" + str(costs[0]))\n",
    "\n",
    "L_layer_model_test(L_layer_model)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73b6e27-df36-47d7-8527-f639b8050efb",
   "metadata": {},
   "source": [
    "#### 02.02.04 Training\n",
    "```python\n",
    "parameters, costs = L_layer_model(train_x, train_y, layers_dims, num_iterations = 2500, print_cost = True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53316f1-1ab1-4324-9020-620d5e0479ec",
   "metadata": {},
   "source": [
    "#### 02.02.05 Predictions\n",
    "```python\n",
    "pred_train = predict(train_x, train_y, parameters)\n",
    "pred_test = predict(test_x, test_y, parameters)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLS",
   "language": "python",
   "name": "dls"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
