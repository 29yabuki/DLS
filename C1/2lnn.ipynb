{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d44c73e-8ad4-444a-acfa-3d386fd786b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb0fa91-f230-434e-a2b6-d4ff5e58f60e",
   "metadata": {},
   "source": [
    "## 01 Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4db22d2-3b1d-4126-93f7-fffca582b492",
   "metadata": {},
   "source": [
    "```python\n",
    "clf = LogisticRegressionCV()\n",
    "clf.fit(X.T, Y.T.ravel())\n",
    "LR_predictions = clf.predict(X.T)\n",
    "accuracy = np.mean(LR_predictions == Y.T.ravel()) * 100\n",
    "\n",
    "print(f'Accuracy of logistic regression: {accuracy:.2f}% (percentage of correctly labelled datapoints)')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dbc39b-6911-45a9-8ed1-26f46e1dd0d3",
   "metadata": {},
   "source": [
    "## 02 Two-layer neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb9fd00-ee22-4895-a11a-628c95aaee79",
   "metadata": {},
   "source": [
    "General methodology to build a Neural Network:\n",
    "1. Define the neural network structure ( # of input units,  # of hidden units, etc). \n",
    "2. Initialize the model's parameters\n",
    "3. Loop:\n",
    "    - Implement forward propagation\n",
    "    - Compute loss\n",
    "    - Implement backward propagation to get the gradients\n",
    "    - Update parameters (gradient descent)\n",
    "\n",
    "**REMARKS**:\n",
    "- Build helper functions to compute steps 1-3.\n",
    "- Merge them into a single function called `nn_model()`.\n",
    "- Once nn_model is trained and learned the parameters, you can use them to make predictions on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d790dfa1-e072-4752-ab61-5c7a7df8be36",
   "metadata": {},
   "source": [
    "For one example $x^{(i)}$:\n",
    "$$z^{[1] (i)} =  W^{[1]} x^{(i)} + b^{[1]}\\tag{1}$$ \n",
    "$$a^{[1] (i)} = \\tanh(z^{[1] (i)})\\tag{2}$$\n",
    "$$z^{[2] (i)} = W^{[2]} a^{[1] (i)} + b^{[2]}\\tag{3}$$\n",
    "$$\\hat{y}^{(i)} = a^{[2] (i)} = \\sigma(z^{ [2] (i)})\\tag{4}$$\n",
    "$$y^{(i)}_{prediction} = \\begin{cases} 1 & \\mbox{if } a^{[2](i)} > 0.5 \\\\ 0 & \\mbox{otherwise } \\end{cases}\\tag{5}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955d69eb-a761-4c4d-a40a-997095b1c0cb",
   "metadata": {},
   "source": [
    "Given the predictions on all the examples, the cost $J$ is: \n",
    "$$J = - \\frac{1}{m} \\sum\\limits_{i = 0}^{m} \\large\\left(\\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right)  \\large  \\right) \\small \\tag{6}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876a03a7-d8c0-4b37-9368-832fc862145b",
   "metadata": {},
   "source": [
    "### 02.01 Define the structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a59c50-5bae-4030-9ced-7bd11d5e6672",
   "metadata": {},
   "source": [
    "Define three variables:\n",
    "- n_x: the size of the input layer\n",
    "- n_h: the size of the hidden layer\n",
    "- n_y: the size of the output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52fa5fc7-bcfe-433e-9255-51590d6029b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_sizes(X, Y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X: input dataset of shape (input size, number of examples)\n",
    "    Y: labels of shape (output size, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    n_x: the size of the input layer\n",
    "    n_h: the size of the hidden layer\n",
    "    n_y: the size of the output layer\n",
    "    \"\"\"\n",
    "    \n",
    "    n_x = X.shape[0]\n",
    "    n_h = 4\n",
    "    n_y = Y.shape[0]\n",
    "    \n",
    "    return (n_x, n_h, n_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85544b39-c60c-4eae-95b2-9b502b3c7144",
   "metadata": {},
   "source": [
    "### 02.02 Initialize parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfce769-65ca-42ba-b837-d89ec6935ae5",
   "metadata": {},
   "source": [
    "- Initialize the weights matrices with random values via `np.random.randn(a,b) * 0.01`.\n",
    "- Initialize the bias vectors as zeros via `np.zeros((a,b))`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02c987ac-7627-4910-902b-a5b2d8f1c25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x: size of the input layer\n",
    "    n_h: size of the hidden layer\n",
    "    n_y: size of the output layer\n",
    "    \n",
    "    Returns:\n",
    "    params: dictionary containing parameters:\n",
    "                    W1: weight matrix of shape (n_h, n_x)\n",
    "                    b1: bias vector of shape (n_h, 1)\n",
    "                    W2: weight matrix of shape (n_y, n_h)\n",
    "                    b2: bias vector of shape (n_y, 1)\n",
    "    \"\"\"    \n",
    "    \n",
    "    W1 = np.random.randn(n_h, n_x) * 0.01 # shape of (n_h, n_x)\n",
    "    b1 = np.zeros((n_h, 1)) # shape of (n_h, 1)\n",
    "    W2 = np.random.randn(n_y, n_h) * 0.01 # shape of (n_y, n_h)\n",
    "    b2 = np.zeros((n_y, 1)) # (n_y , 1)\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f65eddd-d9e1-45be-9597-adf29e4e1fe4",
   "metadata": {},
   "source": [
    "### 02.03 Forward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cfb38b-54e3-4bf8-9da5-ead16b399b4c",
   "metadata": {},
   "source": [
    "Implementation of `forward_propagation()` using the following equations:\n",
    "$$Z^{[1]} =  W^{[1]} X + b^{[1]}\\tag{1}$$ \n",
    "$$A^{[1]} = \\tanh(Z^{[1]})\\tag{2}$$\n",
    "$$Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}\\tag{3}$$\n",
    "$$\\hat{Y} = A^{[2]} = \\sigma(Z^{[2]})\\tag{4}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd296980-e08c-4194-839e-1d1786fba140",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    X: input data of size (n_x, m)\n",
    "    parameters: dict containing parameters (output of initialization function)\n",
    "    \n",
    "    Returns:\n",
    "    A2: sigmoid output of second activation\n",
    "    cache: dict containing  \"Z1\", \"A1\", \"Z2\" and \"A2\"\n",
    "    \"\"\"\n",
    "\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    \n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = np.tanh(Z1)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    \n",
    "    cache = {\"Z1\": Z1,\n",
    "             \"A1\": A1,\n",
    "             \"Z2\": Z2,\n",
    "             \"A2\": A2}\n",
    "    \n",
    "    return A2, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf89bda-1c60-425c-92ba-f217a5d1ca76",
   "metadata": {},
   "source": [
    "### 02.04 Compute cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff86f585-55d2-48b9-91c5-c50f5ca2fe7b",
   "metadata": {},
   "source": [
    "Compute the cost function as follows:\n",
    "\n",
    "$$J = - \\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(} \\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right) \\large{)} \\small\\tag{13}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2744bd05-9ca8-46c4-a37a-33c9387930d2",
   "metadata": {},
   "source": [
    "One way to implement one part of the equation without for loops: $- \\sum\\limits_{i=1}^{m}  y^{(i)}\\log(a^{[2](i)})$:\n",
    "```python\n",
    "logprobs = np.multiply(np.log(A2),Y)\n",
    "cost = - np.sum(logprobs)\n",
    "```\n",
    "\n",
    "- Use np.multiply + np.sum, or np.dot.\n",
    "- np.multiply with np.sum returns a float, while np.dot returns a 2D array.\n",
    "- Use np.squeeze() or float() to reduce the result to a scalar if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5caa132e-a279-4e5b-9e34-0eed371194e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(A2, Y):\n",
    "    \"\"\"\n",
    "    Computes the cross-entropy cost.\n",
    "    \n",
    "    Arguments:\n",
    "    A2: The sigmoid output of the second activation, of shape (1, number of examples)\n",
    "    Y: \"true\" labels vector of shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost given equation\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1] # number of examples\n",
    "\n",
    "    logprobs = np.multiply(Y, np.log(A2)) + np.multiply((1-Y), np.log(1-A2))\n",
    "    cost = -(1/m)*np.sum(logprobs)\n",
    "        \n",
    "    cost = float(np.squeeze(cost))  # makes sure cost is the dimension we expect. \n",
    "                                    # E.g., turns [[17]] into 17 \n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0031f645-ccf2-4555-87c1-023d4fc0c52a",
   "metadata": {},
   "source": [
    "### 02.05 Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b6eed9-ebbd-4a12-8fa7-b1a0e90a8152",
   "metadata": {},
   "source": [
    "Using the cache computed during forward propagation, you can now implement backward propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7316cfc6-2ce3-4f3d-9f6c-f9f4b5c84fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(parameters, cache, X, Y):\n",
    "    \"\"\"\n",
    "    \n",
    "    Arguments:\n",
    "    parameters: dict containing our parameters \n",
    "    cache: a dict containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n",
    "    X: input data of shape (2, number of examples)\n",
    "    Y: \"true\" labels vector of shape (1, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    grads: dict containing grads with respect to different parameters\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters['W2']\n",
    "\n",
    "    A1 = cache['A1']\n",
    "    A2 = cache['A2']\n",
    "    \n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = (1/m) * np.dot(dZ2, A1.T)\n",
    "    db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    dZ1 = np.dot(W2.T, dZ2) * (1 - np.power(A1, 2))\n",
    "    dW1 = (1/m) * np.dot(dZ1, X.T)\n",
    "    db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    \n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2}\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e6c463-a414-4f54-819a-3269c6dddcdb",
   "metadata": {},
   "source": [
    "### 02.06 Update parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9b412a-8b5f-473d-bdf6-c545b7aa5553",
   "metadata": {},
   "source": [
    "Implement gradient descent to update (W1, b1, W2, b2) using their gradients (dW1, db1, dW2, db2).\n",
    "\n",
    "General gradient descent rule: $\\theta = \\theta - \\alpha \\frac{\\partial J }{ \\partial \\theta }$ where $\\alpha$ is the learning rate and $\\theta$ represents a parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8d6b396-a5b6-4507-be12-4935a286c2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate = 1.2):\n",
    "    \"\"\"\n",
    "    Updates parameters using the gradient descent update rule given above\n",
    "    \n",
    "    Arguments:\n",
    "    parameters: python dictionary containing your parameters \n",
    "    grads: python dictionary containing your gradients \n",
    "    \n",
    "    Returns:\n",
    "    parameters: python dictionary containing your updated parameters \n",
    "    \"\"\"\n",
    "    W1 = copy.deepcopy(parameters['W1'])\n",
    "    b1 = copy.deepcopy(parameters['b1'])\n",
    "    W2 = copy.deepcopy(parameters['W2'])\n",
    "    b2 = copy.deepcopy(parameters['b2'])\n",
    "    \n",
    "    dW1 = grads['dW1']\n",
    "    db1 = grads['db1']\n",
    "    dW2 = grads['dW2']\n",
    "    db2 = grads['db2']\n",
    "    \n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b397ced-8ae2-4301-9ed1-97378877fb51",
   "metadata": {},
   "source": [
    "### 02.07 Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e37b49f-f74d-42e7-9288-c101545cc638",
   "metadata": {},
   "source": [
    "The neural network model has to use the previous functions in the right order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a9b5b8b-7972-4343-baad-36336ed00b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model(X, Y, n_h, num_iterations = 10000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X: dataset of shape (2, number of examples)\n",
    "    Y: labels of shape (1, number of examples)\n",
    "    n_h: size of the hidden layer\n",
    "    num_iterations: num of iterations in gradient descent loop\n",
    "    print_cost: if True, print the cost every 1000 iterations\n",
    "    \n",
    "    Returns:\n",
    "    parameters: parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    n_x = layer_sizes(X, Y)[0]\n",
    "    n_y = layer_sizes(X, Y)[2]\n",
    "    \n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "        A2, cache = forward_propagation(X, parameters)\n",
    "        cost = compute_cost(A2, Y)\n",
    "        grads = backward_propagation(parameters, cache, X, Y)\n",
    "        parameters = update_parameters(parameters, grads)\n",
    "\n",
    "        # Print the cost every 1000 iterations\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96e6610-a5fb-4f44-a1b8-cb644809e7e6",
   "metadata": {},
   "source": [
    "### 02.08 Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc56148a-ef1a-40c9-8c4e-64029cfdea41",
   "metadata": {},
   "source": [
    "Use forward propagation to predict results.\n",
    "\n",
    "predictions = $y_{prediction} = \\mathbb \\{\\text{{activation > 0.5}}\\} = \\begin{cases}\n",
    "      1 & \\text{if}\\ \\text{activation} > 0.5 \\\\\n",
    "      0 & \\text{otherwise}\n",
    "    \\end{cases}$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6b3bc32-9de8-437f-804b-e44b4e71416d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(parameters, X):\n",
    "    \"\"\"\n",
    "    Using the learned parameters, predicts a class for each example in X\n",
    "    \n",
    "    Arguments:\n",
    "    parameters: python dictionary containing your parameters \n",
    "    X: input data of size (n_x, m)\n",
    "    \n",
    "    Returns\n",
    "    predictions: vector of predictions of our model (red: 0 / blue: 1)\n",
    "    \"\"\"\n",
    "\n",
    "    A2, cache = forward_propagation(X, parameters)\n",
    "    predictions = A2 > 0.5\n",
    "    \n",
    "    return predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLS",
   "language": "python",
   "name": "dls"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
